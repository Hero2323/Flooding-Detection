{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rorod\\miniforge3\\envs\\new_python_nlp_2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "from torch.utils.data.dataset import Dataset \n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms,models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.metrics import f1_score   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   #must same as here\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(), # data augmentation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalization\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   #must same as here\n",
    "     transforms.CenterCrop((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   #must same as here\n",
    "     transforms.CenterCrop((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"Data/train/\"\n",
    "test_dir = \"Data/test/\"\n",
    "val_dir= \"Data/val/\"\n",
    "train_flooded_dir = \"Data/train/flooded/\"\n",
    "train_non_flooded_dir = \"Data/train/non-flooded/\"\n",
    "val_flooded_dir = \"Data/val/flooded/\"\n",
    "val_non_flooded_dir = \"Data/val/non-flooded/\"\n",
    "test_flooded_dir = \"Data/test/flooded/\"\n",
    "test_non_flooded_dir = \"Data/test/non-flooded/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(train_dir, transforms_train)\n",
    "test_dataset = ImageFolder(test_dir, transforms_test)\n",
    "val_dataset = ImageFolder(val_dir, transforms_val)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=12, shuffle=True, num_workers=8)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=12, shuffle=True, num_workers=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=12, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 644\n",
      "Test dataset size: 140\n",
      "Val dataset size: 138\n",
      "Class names: ['flooded', 'non-flooded']\n"
     ]
    }
   ],
   "source": [
    "print('Train dataset size:', len(train_dataset))\n",
    "print('Test dataset size:', len(test_dataset))\n",
    "print('Val dataset size:', len(val_dataset))\n",
    "class_names = train_dataset.classes\n",
    "print('Class names:', class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rorod\\miniforge3\\envs\\new_python_nlp_2\\lib\\site-packages\\torchvision\\models\\_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "c:\\Users\\rorod\\miniforge3\\envs\\new_python_nlp_2\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=RegNet_X_1_6GF_Weights.IMAGENET1K_V1`. You can also use `weights=RegNet_X_1_6GF_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.regnet_x_1_6gf(pretrained=True)   #load vgg16 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = model.fc.in_features     #extract fc layers features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(num_features, 2) #(num_of_class == 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  #(set loss function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 running\n",
      "[Train #0] Loss: 0.4875 Acc: 75.1553% Time: 103.8501s\n",
      "F1-Score 0.7553516819571865\n",
      "[Val #0] Loss: 0.1828 Acc: 97.1014% Time: 115.2358s\n",
      "F1-Score 0.9714285714285714\n",
      "Epoch 1 running\n",
      "[Train #1] Loss: 0.2676 Acc: 88.6646% Time: 218.2594s\n",
      "F1-Score 0.8864696734059097\n",
      "[Val #1] Loss: 0.1041 Acc: 98.5507% Time: 228.8018s\n",
      "F1-Score 0.9857142857142858\n",
      "Epoch 2 running\n",
      "[Train #2] Loss: 0.2162 Acc: 91.3044% Time: 331.9037s\n",
      "F1-Score 0.9125000000000001\n",
      "[Val #2] Loss: 0.0773 Acc: 98.5507% Time: 342.1626s\n",
      "F1-Score 0.9855072463768116\n",
      "Epoch 3 running\n",
      "[Train #3] Loss: 0.1381 Acc: 95.3416% Time: 445.6123s\n",
      "F1-Score 0.9539877300613496\n",
      "[Val #3] Loss: 0.0583 Acc: 98.5507% Time: 456.4333s\n",
      "F1-Score 0.9855072463768116\n",
      "Epoch 4 running\n",
      "[Train #4] Loss: 0.1654 Acc: 93.9441% Time: 559.8204s\n",
      "F1-Score 0.9402756508422665\n",
      "[Val #4] Loss: 0.0663 Acc: 98.5507% Time: 570.6444s\n",
      "F1-Score 0.9855072463768116\n",
      "Epoch 5 running\n",
      "[Train #5] Loss: 0.1266 Acc: 95.0311% Time: 676.1151s\n",
      "F1-Score 0.9503105590062112\n",
      "[Val #5] Loss: 0.0466 Acc: 98.5507% Time: 687.4308s\n",
      "F1-Score 0.9855072463768116\n",
      "Epoch 6 running\n",
      "[Train #6] Loss: 0.1142 Acc: 95.6522% Time: 789.7064s\n",
      "F1-Score 0.9566563467492261\n",
      "[Val #6] Loss: 0.0577 Acc: 98.5507% Time: 800.1337s\n",
      "F1-Score 0.9857142857142858\n",
      "Epoch 7 running\n",
      "[Train #7] Loss: 0.1493 Acc: 94.2547% Time: 902.7527s\n",
      "F1-Score 0.9433384379785605\n",
      "[Val #7] Loss: 0.0751 Acc: 98.5507% Time: 913.1805s\n",
      "F1-Score 0.9852941176470589\n",
      "Epoch 8 running\n",
      "[Train #8] Loss: 0.1389 Acc: 94.0994% Time: 1016.4442s\n",
      "F1-Score 0.940809968847352\n",
      "[Val #8] Loss: 0.0436 Acc: 99.2754% Time: 1027.2306s\n",
      "F1-Score 0.9928057553956835\n",
      "Epoch 9 running\n",
      "[Train #9] Loss: 0.0992 Acc: 95.9627% Time: 1130.8301s\n",
      "F1-Score 0.96\n",
      "[Val #9] Loss: 0.0511 Acc: 98.5507% Time: 1141.1964s\n",
      "F1-Score 0.9855072463768116\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10   #(set no of epochs)\n",
    "start_time = time.time() #(for showing time)\n",
    "\n",
    "for epoch in range(num_epochs): #(loop for every epoch)\n",
    "    print(\"Epoch {} running\".format(epoch)) #(printing message)\n",
    "    \"\"\" Training Phase \"\"\"\n",
    "    model.train()    #(training model)\n",
    "    running_loss = 0.   #(set loss 0)\n",
    "    running_corrects = 0 \n",
    "    # load a batch data of images\n",
    "    targets = []\n",
    "    results = []\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device) \n",
    "        # forward inputs and get output\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        results.append(preds)\n",
    "        targets.append(labels)\n",
    "        # get loss value and update the network weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    results =torch.cat(results, dim=0)\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "\n",
    "    results = results.to('cpu').numpy().flatten()\n",
    "    targets = targets.to('cpu').numpy().flatten()\n",
    "\n",
    "    f1_value =  f1_score(results, targets)\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = running_corrects / len(train_dataset) * 100.\n",
    "    print('[Train #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() -start_time))\n",
    "    print('F1-Score', f1_value)\n",
    "    \"\"\" Val Phase \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.\n",
    "        running_corrects = 0\n",
    "        targets = []\n",
    "        results = []\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            results.append(preds)\n",
    "            targets.append(labels)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        epoch_loss = running_loss / len(val_dataset)\n",
    "        epoch_acc = running_corrects / len(val_dataset) * 100.\n",
    "        results = torch.cat(results, dim=0)\n",
    "        targets =torch.cat(targets, dim=0)\n",
    "        results = results.to('cpu').numpy().flatten()\n",
    "        targets = targets.to('cpu').numpy().flatten()\n",
    "        f1_value =  f1_score(results, targets)\n",
    "        print('[Val #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time()- start_time))\n",
    "        print('F1-Score', f1_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score 1.0\n",
      "[Test ] Loss: 0.0418 Acc: 100.0000% Time: 1152.3409s\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    running_loss = 0.\n",
    "    running_corrects = 0\n",
    "    targets = []\n",
    "    results = []\n",
    "    for inputs, labels in test_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        results.append(preds)\n",
    "        targets.append(labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    epoch_loss = running_loss / len(test_dataset)\n",
    "    epoch_acc = running_corrects / len(test_dataset) * 100.\n",
    "    results = torch.cat(results, dim=0)\n",
    "    targets =torch.cat(targets, dim=0)\n",
    "    results = results.to('cpu').numpy().flatten()\n",
    "    targets = targets.to('cpu').numpy().flatten()\n",
    "    f1_value =  f1_score(results, targets)\n",
    "    print('F1-Score', f1_value)\n",
    "    print('[Test ] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format( epoch_loss, epoch_acc, time.time()- start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
