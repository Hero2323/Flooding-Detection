{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rorod\\miniforge3\\envs\\new_python_nlp_2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "from torch.utils.data.dataset import Dataset \n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms,models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.metrics import f1_score   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   #must same as here\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(), # data augmentation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalization\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   #must same as here\n",
    "     transforms.CenterCrop((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   #must same as here\n",
    "     transforms.CenterCrop((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"Data/train/\"\n",
    "test_dir = \"Data/test/\"\n",
    "val_dir= \"Data/val/\"\n",
    "train_flooded_dir = \"Data/train/flooded/\"\n",
    "train_non_flooded_dir = \"Data/train/non-flooded/\"\n",
    "val_flooded_dir = \"Data/val/flooded/\"\n",
    "val_non_flooded_dir = \"Data/val/non-flooded/\"\n",
    "test_flooded_dir = \"Data/test/flooded/\"\n",
    "test_non_flooded_dir = \"Data/test/non-flooded/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(train_dir, transforms_train)\n",
    "test_dataset = ImageFolder(test_dir, transforms_test)\n",
    "val_dataset = ImageFolder(val_dir, transforms_val)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=12, shuffle=True, num_workers=8)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=12, shuffle=True, num_workers=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=12, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 644\n",
      "Test dataset size: 140\n",
      "Val dataset size: 138\n",
      "Class names: ['flooded', 'non-flooded']\n"
     ]
    }
   ],
   "source": [
    "print('Train dataset size:', len(train_dataset))\n",
    "print('Test dataset size:', len(test_dataset))\n",
    "print('Val dataset size:', len(val_dataset))\n",
    "class_names = train_dataset.classes\n",
    "print('Class names:', class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rorod\\miniforge3\\envs\\new_python_nlp_2\\lib\\site-packages\\torchvision\\models\\_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "c:\\Users\\rorod\\miniforge3\\envs\\new_python_nlp_2\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to C:\\Users\\rorod/.cache\\torch\\hub\\checkpoints\\mobilenet_v2-b0353104.pth\n",
      "100%|██████████| 13.6M/13.6M [00:07<00:00, 1.93MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = models.mobilenet_v2(pretrained=True)   #load mobile net  model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = model.last_channel    #extract fc layers features\n",
    "model.classifier[1] = nn.Linear(num_features, 2) #(num_of_class == 2)\n",
    "model = model.to(device) \n",
    "criterion = nn.CrossEntropyLoss()  #(set loss function)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 running\n",
      "[Train #0] Loss: 0.3899 Acc: 81.3665% Time: 87.2137s\n",
      "F1-Score 0.8130841121495327\n",
      "[Val #0] Loss: 0.1417 Acc: 92.7536% Time: 100.2447s\n",
      "F1-Score 0.9324324324324325\n",
      "Epoch 1 running\n",
      "[Train #1] Loss: 0.1938 Acc: 92.7019% Time: 185.5993s\n",
      "F1-Score 0.9264475743348982\n",
      "[Val #1] Loss: 0.2221 Acc: 88.4058% Time: 197.0952s\n",
      "F1-Score 0.8933333333333333\n",
      "Epoch 2 running\n",
      "[Train #2] Loss: 0.2577 Acc: 90.2174% Time: 287.2211s\n",
      "F1-Score 0.9044006069802731\n",
      "[Val #2] Loss: 0.1175 Acc: 95.6522% Time: 296.4632s\n",
      "F1-Score 0.9545454545454545\n",
      "Epoch 3 running\n",
      "[Train #3] Loss: 0.1861 Acc: 93.9441% Time: 381.5314s\n",
      "F1-Score 0.9395348837209302\n",
      "[Val #3] Loss: 0.0912 Acc: 98.5507% Time: 390.9763s\n",
      "F1-Score 0.9852941176470589\n",
      "Epoch 4 running\n",
      "[Train #4] Loss: 0.1336 Acc: 94.5652% Time: 476.7933s\n",
      "F1-Score 0.9460708782742681\n",
      "[Val #4] Loss: 0.0372 Acc: 97.8261% Time: 486.1051s\n",
      "F1-Score 0.9784172661870504\n",
      "Epoch 5 running\n",
      "[Train #5] Loss: 0.1097 Acc: 95.3416% Time: 571.3596s\n",
      "F1-Score 0.9538461538461539\n",
      "[Val #5] Loss: 0.0430 Acc: 98.5507% Time: 580.2826s\n",
      "F1-Score 0.9857142857142858\n",
      "Epoch 6 running\n",
      "[Train #6] Loss: 0.1200 Acc: 94.8758% Time: 665.8446s\n",
      "F1-Score 0.9488372093023255\n",
      "[Val #6] Loss: 0.0530 Acc: 98.5507% Time: 674.9179s\n",
      "F1-Score 0.9852941176470589\n",
      "Epoch 7 running\n",
      "[Train #7] Loss: 0.1832 Acc: 93.3230% Time: 761.4424s\n",
      "F1-Score 0.9337442218798152\n",
      "[Val #7] Loss: 0.0659 Acc: 97.1014% Time: 770.9685s\n",
      "F1-Score 0.9714285714285714\n",
      "Epoch 8 running\n",
      "[Train #8] Loss: 0.1103 Acc: 95.8074% Time: 856.1821s\n",
      "F1-Score 0.958139534883721\n",
      "[Val #8] Loss: 0.0314 Acc: 99.2754% Time: 865.1265s\n",
      "F1-Score 0.9927007299270074\n",
      "Epoch 9 running\n",
      "[Train #9] Loss: 0.1149 Acc: 96.1180% Time: 947.3529s\n",
      "F1-Score 0.9613601236476043\n",
      "[Val #9] Loss: 0.0298 Acc: 100.0000% Time: 955.8000s\n",
      "F1-Score 1.0\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10   #(set no of epochs)\n",
    "start_time = time.time() #(for showing time)\n",
    "\n",
    "for epoch in range(num_epochs): #(loop for every epoch)\n",
    "    print(\"Epoch {} running\".format(epoch)) #(printing message)\n",
    "    \"\"\" Training Phase \"\"\"\n",
    "    model.train()    #(training model)\n",
    "    running_loss = 0.   #(set loss 0)\n",
    "    running_corrects = 0 \n",
    "    # load a batch data of images\n",
    "    targets = []\n",
    "    results = []\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device) \n",
    "        # forward inputs and get output\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        results.append(preds)\n",
    "        targets.append(labels)\n",
    "        # get loss value and update the network weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    results =torch.cat(results, dim=0)\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "\n",
    "    results = results.to('cpu').numpy().flatten()\n",
    "    targets = targets.to('cpu').numpy().flatten()\n",
    "\n",
    "    f1_value =  f1_score(results, targets)\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = running_corrects / len(train_dataset) * 100.\n",
    "    print('[Train #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() -start_time))\n",
    "    print('F1-Score', f1_value)\n",
    "    \"\"\" Val Phase \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.\n",
    "        running_corrects = 0\n",
    "        targets = []\n",
    "        results = []\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            results.append(preds)\n",
    "            targets.append(labels)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        epoch_loss = running_loss / len(val_dataset)\n",
    "        epoch_acc = running_corrects / len(val_dataset) * 100.\n",
    "        results = torch.cat(results, dim=0)\n",
    "        targets =torch.cat(targets, dim=0)\n",
    "        results = results.to('cpu').numpy().flatten()\n",
    "        targets = targets.to('cpu').numpy().flatten()\n",
    "        f1_value =  f1_score(results, targets)\n",
    "        print('[Val #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time()- start_time))\n",
    "        print('F1-Score', f1_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score 0.9705882352941176\n",
      "[Test ] Loss: 0.0837 Acc: 97.1429% Time: 986.8978s\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    running_loss = 0.\n",
    "    running_corrects = 0\n",
    "    targets = []\n",
    "    results = []\n",
    "    for inputs, labels in test_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        results.append(preds)\n",
    "        targets.append(labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    epoch_loss = running_loss / len(test_dataset)\n",
    "    epoch_acc = running_corrects / len(test_dataset) * 100.\n",
    "    results = torch.cat(results, dim=0)\n",
    "    targets =torch.cat(targets, dim=0)\n",
    "    results = results.to('cpu').numpy().flatten()\n",
    "    targets = targets.to('cpu').numpy().flatten()\n",
    "    f1_value =  f1_score(results, targets)\n",
    "    print('F1-Score', f1_value)\n",
    "    print('[Test ] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format( epoch_loss, epoch_acc, time.time()- start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
